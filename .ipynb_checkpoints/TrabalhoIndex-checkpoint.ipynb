{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports e declarações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "col = [\"Abandono\",\"Cancelamento\",\"Classificado e Não Matriculado\",\"Desligamento\",\"Transferido\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparação, limpeza e agregação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparação e limpeza do dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleardb1(data):\n",
    "    data.columns = data.loc[0]\n",
    "    data.drop(data.index[:1], inplace=True)\n",
    "    data.drop(data.index[-1], inplace=True)\n",
    "    data[\"Campus\"].fillna(method='ffill', inplace=True)\n",
    "    data.rename({\"NOME_CURSO\": \"curso\", \"Campus\": \"campus\"}, axis=1, inplace=True)\n",
    "    \n",
    "    for i in col:\n",
    "        if i not in data.columns:\n",
    "            data[i] = np.nan\n",
    "    for i in col:\n",
    "        data[i].fillna(0, inplace=True)\n",
    "    data.dropna(axis=1, inplace=True)\n",
    "    \n",
    "    data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carregando as databases e limpando com os metodos definidos anteriormente\n",
    "d061 = pd.read_csv(\"./data/evasao - 061.csv\", encoding = \"utf-8\")\n",
    "cleardb1(d061)\n",
    "d061.drop(columns=[\"Nº de alunos\"], inplace=True)\n",
    "\n",
    "d062 = pd.read_csv(\"./data/evasao - 062.csv\", encoding = \"utf-8\")\n",
    "cleardb1(d062)\n",
    "d062.drop(columns=[\"Nº de alunos\"], inplace=True)\n",
    "\n",
    "d071 = pd.read_csv(\"./data/evasao - 071.csv\", encoding = \"utf-8\")\n",
    "cleardb1(d071)\n",
    "d071.drop(columns=[\"Nº alunos\"], inplace=True)\n",
    "\n",
    "d072 = pd.read_csv(\"./data/evasao - 072.csv\", encoding = \"utf-8\")\n",
    "cleardb1(d072)\n",
    "d072.drop(columns=[\"Nº de alunos\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparação e limpeza do dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleardb2(data, lastline):\n",
    "    data.columns = data.loc[1]\n",
    "    data.drop(data.index[:2], inplace=True)\n",
    "    data.drop(data.index[lastline], inplace=True)\n",
    "    data[\"NOME_CURSO\"].fillna(method='ffill', inplace=True)\n",
    "    try:\n",
    "        data[\"Campus\"].fillna(method='ffill', inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    data.dropna(1, inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data = split(data)\n",
    "    data = pivoting(data)\n",
    "    return data\n",
    "    \n",
    "def split(data):\n",
    "    \n",
    "    ocorrencia = data[[\"Ocorrência/forma de evasão\"]]\n",
    "    ocorrencia = pd.DataFrame(ocorrencia[\"Ocorrência/forma de evasão\"].str.split(\"=\").tolist())\n",
    "    data = pd.concat([data, ocorrencia], axis=1, join_axes=[d081.index])\n",
    "    \n",
    "    data.drop(columns=[\"Ocorrência/forma de evasão\"], inplace=True)\n",
    "    data.rename({\"NOME_CURSO\": \"curso\", 0:\"tipo\", 1:\"quantidade\"}, axis=1, inplace=True)\n",
    "    data.quantidade = data.quantidade.apply(pd.to_numeric)\n",
    "    return data\n",
    "\n",
    "def pivoting(data):\n",
    "    data.dropna(inplace=True)\n",
    "    data1 = data.pivot(index=\"curso\", columns='tipo', values=\"quantidade\")\n",
    "    data1.fillna(0, inplace=True)\n",
    "    data1.reset_index(inplace=True)\n",
    "    if(\"Campus\" in data):\n",
    "        data.rename({\"Campus\":\"campus\"}, axis=1, inplace=True)\n",
    "        data1 = pd.concat([data1, data.campus], axis=1)\n",
    "    else:\n",
    "        data[\"campus\"] = pd.Series(['' for i in range(len(data['curso']))])\n",
    "        data1 = pd.concat([data1, data.campus], axis=1)\n",
    "    data1.dropna(axis=0, inplace=True)\n",
    "    return data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carregando as databases e limpando com os metodos definidos anteriormente\n",
    "d081 = pd.read_csv(\"./data/evasao - 081.csv\", encoding = \"utf-8\")\n",
    "d081 = cleardb2(d081,-4)\n",
    "\n",
    "d082 = pd.read_csv(\"./data/evasao - 082.csv\", encoding = \"utf-8\")\n",
    "d082 = cleardb2(d082, -5)\n",
    "\n",
    "d091 = pd.read_csv(\"./data/evasao - 091.csv\", encoding = \"utf-8\")\n",
    "d091 = cleardb2(d091, -8)\n",
    "\n",
    "d092 = pd.read_csv(\"./data/evasao - 092.csv\", encoding = \"utf-8\")\n",
    "d092 = cleardb2(d092, -6)\n",
    "\n",
    "d101 = pd.read_csv(\"./data/evasao - 101.csv\", encoding = \"utf-8\")\n",
    "d101 = cleardb2(d101, -8)\n",
    "\n",
    "d102 = pd.read_csv(\"./data/evasao - 102.csv\", encoding = \"utf-8\")\n",
    "d102 = cleardb2(d102,-7)\n",
    "\n",
    "d111 = pd.read_csv(\"./data/evasao - 111.csv\", encoding = \"utf-8\")\n",
    "d111 = cleardb2(d111,-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#com base com todas as planilhas anteriores pega todos os cursos e seus campus\n",
    "datasets = [d061[[\"curso\", \"campus\"]],d062[[\"curso\", \"campus\"]],d071[[\"curso\", \"campus\"]],d072[[\"curso\", \"campus\"]],\n",
    "            d081[[\"curso\", \"campus\"]],d082[[\"curso\", \"campus\"]],d091[[\"curso\", \"campus\"]],d092[[\"curso\", \"campus\"]],\n",
    "            d101[[\"curso\", \"campus\"]],d102[[\"curso\", \"campus\"]],d111[[\"curso\", \"campus\"]]]\n",
    "\n",
    "cursos = pd.concat(datasets)\n",
    "cursos = cursos[cursos.campus != '']\n",
    "cursos.drop_duplicates(inplace=True)\n",
    "cursos.sort_values(by=\"campus\", inplace=True)\n",
    "cursos.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finalizando a limpesa dos datasets agregando o campus na tabela dos cursos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [d061,d062,d071,d072,d081,d082,d091,d092,d101,d102,d111]\n",
    "for i in range(len(datasets)):\n",
    "    if(\"campus\" in datasets[i]):\n",
    "        datasets[i].drop(columns=[\"campus\"], inplace=True)\n",
    "    datasets[i] = pd.merge(datasets[i], cursos, left_on=\"curso\", right_on=\"curso\")\n",
    "    datasets[i].sort_values(by=\"campus\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
